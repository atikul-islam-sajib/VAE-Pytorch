{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import zipfile\n",
    "import traceback\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import joblib\n",
    "\n",
    "\n",
    "def config():\n",
    "    with open(\"../../config.yml\", \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "def dump(value=None, filename=None):\n",
    "    if (value is not None) and (filename is not None):\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "\n",
    "\n",
    "def load(filename=None):\n",
    "    if filename is not None:\n",
    "        return joblib.load(filename=filename)\n",
    "\n",
    "\n",
    "class CustomException(Exception):\n",
    "    def __init__(self, message=None):\n",
    "        super().__init__(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(\n",
    "        self, image_path=None, channels=3, image_size=256, batch_size=4, split_size=0.20\n",
    "    ):\n",
    "        self.image_path = image_path\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.split_size = split_size\n",
    "\n",
    "        self.actual = []\n",
    "        self.target = []\n",
    "\n",
    "    def unzip_folder(self):\n",
    "        self.raw_data_path = config()[\"path\"][\"RAW_DATA_PATH\"]\n",
    "\n",
    "        if os.path.exists(self.raw_data_path):\n",
    "            with zipfile.ZipFile(self.image_path, \"r\") as zip_file:\n",
    "                zip_file.extractall(path=os.path.join(self.raw_data_path))\n",
    "\n",
    "            print(\n",
    "                \"Unzip is done successfully and stoed in the path {}\".format(\n",
    "                    os.path.join(self.raw_data_path, \"dataset\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise CustomException(\"Raw data path does not exist\".capitalize())\n",
    "\n",
    "    def transforms(self):\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((self.image_size, self.image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.CenterCrop((self.image_size, self.image_size)),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def split_dataset(self, X, y):\n",
    "        if isinstance(X, list) and isinstance(y, list):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=self.split_size, random_state=42\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"X_train\": X_train,\n",
    "                \"X_test\": X_test,\n",
    "                \"y_train\": y_train,\n",
    "                \"y_test\": y_test,\n",
    "            }\n",
    "        else:\n",
    "            raise CustomException(\"X and y should be list\".capitalize())\n",
    "\n",
    "    def extract_features(self):\n",
    "        self.directory = os.path.join(config()[\"path\"][\"RAW_DATA_PATH\"], \"dataset\")\n",
    "        self.X = os.path.join(config()[\"path\"][\"RAW_DATA_PATH\"], \"dataset\", \"X\")\n",
    "        self.y = os.path.join(config()[\"path\"][\"RAW_DATA_PATH\"], \"dataset\", \"y\")\n",
    "\n",
    "        for image in tqdm(os.listdir(self.X)):\n",
    "            if (image is not None) and (image in os.listdir(self.y)):\n",
    "                self.imageX = os.path.join(self.X, image)\n",
    "                self.imagey = os.path.join(self.y, image)\n",
    "\n",
    "                self.imageX = cv2.imread(filename=self.imageX, flags=cv2.IMREAD_COLOR)\n",
    "                self.imagey = cv2.imread(filename=self.imagey, flags=cv2.IMREAD_COLOR)\n",
    "\n",
    "                self.imageX = cv2.cvtColor(self.imageX, cv2.COLOR_BGR2RGB)\n",
    "                self.imagey = cv2.cvtColor(self.imagey, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                self.imageX = Image.fromarray(self.imageX)\n",
    "                self.imagey = Image.fromarray(self.imagey)\n",
    "\n",
    "                self.imageX = self.transforms()(self.imageX)\n",
    "                self.imagey = self.transforms()(self.imagey)\n",
    "\n",
    "                self.actual.append(self.imageX)\n",
    "                self.target.append(self.imagey)\n",
    "\n",
    "        assert len(self.actual) == len(self.target)\n",
    "\n",
    "        try:\n",
    "            dataset = self.split_dataset(X=self.actual, y=self.target)\n",
    "\n",
    "        except CustomException as e:\n",
    "            print(\"An error occured: \", e)\n",
    "            traceback.print_exc()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"An error occured: \", e)\n",
    "            traceback.print_exc()\n",
    "\n",
    "        else:\n",
    "            print(\"Feature extracted successfully\".capitalize())\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        self.dataset = self.extract_features()\n",
    "        self.processed_data_path = config()[\"path\"][\"PROCESSED_DATA_PATH\"]\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            dataset=list(zip(self.dataset[\"X_train\"], list(self.dataset[\"y_train\"]))),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        self.valid_dataloader = DataLoader(\n",
    "            dataset=list(zip(self.dataset[\"X_test\"], list(self.dataset[\"y_test\"]))),\n",
    "            batch_size=self.batch_size * self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        for value, filename in [\n",
    "            (self.train_dataloader, \"train_dataloader.pkl\"),\n",
    "            (self.valid_dataloader, \"valid_dataloader.pkl\"),\n",
    "        ]:\n",
    "            dump(value=value, filename=os.path.join(self.processed_data_path, filename))\n",
    "\n",
    "        print(\n",
    "            \"DataLoader created successfully and stored in the path {}\".capitalize().format(\n",
    "                self.processed_data_path\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_images():\n",
    "        processed_data_path = config()[\"path\"][\"PROCESSED_DATA_PATH\"]\n",
    "\n",
    "        valid_dataloader = load(\n",
    "            filename=os.path.join(processed_data_path, \"valid_dataloader.pkl\")\n",
    "        )\n",
    "\n",
    "        X, y = next(iter(valid_dataloader))\n",
    "\n",
    "        number_of_rows = X.size(0) // 2\n",
    "        number_of_columns = X.size(0) // number_of_rows\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "\n",
    "        for index, image in enumerate(X):\n",
    "            imageX = image.permute(1, 2, 0).detach().numpy()\n",
    "            imagey = y[index].permute(1, 2, 0).detach().numpy()\n",
    "\n",
    "            imageX = (imageX - imageX.min()) / (imageX.max() - imageX.min())\n",
    "            imagey = (imagey - imagey.min()) / (imagey.max() - imagey.min())\n",
    "\n",
    "            plt.subplot(2 * number_of_rows, 2 * number_of_columns, 2 * index + 1)\n",
    "            plt.title(\"actual\".capitalize())\n",
    "            plt.imshow(imageX)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.subplot(2 * number_of_rows, 2 * number_of_columns, 2 * index + 2)\n",
    "            plt.title(\"target\".capitalize())\n",
    "            plt.imshow(imagey)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config()[\"path\"][\"FILES_PATH\"], \"images.png\"))\n",
    "        plt.show()\n",
    "\n",
    "        print(\n",
    "            \"Images saved in the path {}\".format(\n",
    "                config()[\"path\"][\"FILES_PATH\"]\n",
    "            ).capitalize()\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def details_dataset():\n",
    "        processed_data_path = config()[\"path\"][\"PROCESSED_DATA_PATH\"]\n",
    "\n",
    "        train_dataloader = load(\n",
    "            filename=os.path.join(processed_data_path, \"train_dataloader.pkl\")\n",
    "        )\n",
    "        valid_dataloader = load(\n",
    "            filename=os.path.join(processed_data_path, \"valid_dataloader.pkl\")\n",
    "        )\n",
    "\n",
    "        trainX, trainY = next(iter(train_dataloader))\n",
    "        validX, validY = next(iter(valid_dataloader))\n",
    "\n",
    "        dataframe = pd.DataFrame(\n",
    "            {\n",
    "                \"total_data(X)\": [str(sum(X.size(0) for X, _ in train_dataloader))],\n",
    "                \"total_data(y)\": [str(sum(X.size(0) for X, _ in valid_dataloader))],\n",
    "                \"total_data(X+y)\": [\n",
    "                    str(\n",
    "                        sum(X.size(0) for X, _ in train_dataloader)\n",
    "                        + sum(X.size(0) for X, _ in valid_dataloader)\n",
    "                    )\n",
    "                ],\n",
    "                \"train_image_size(X)\": [str(trainX.size())],\n",
    "                \"valid_image_size(X)\": [str(validX.size())],\n",
    "            },\n",
    "            index=[\"details\".capitalize()],\n",
    "        )\n",
    "\n",
    "        dataframe.to_csv(os.path.join(config()[\"path\"][\"FILES_PATH\"], \"details.csv\"))\n",
    "\n",
    "        print(\n",
    "            \"dataset details saved in the path {}\".format(\n",
    "                config()[\"path\"][\"FILES_PATH\"]\n",
    "            ).capitalize()\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = Loader(image_path=\"./data/raw/dataset1.zip\")\n",
    "\n",
    "    loader.unzip_folder()\n",
    "    loader.create_dataloader()\n",
    "\n",
    "    Loader.plot_images()\n",
    "    Loader.details_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=128):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.kernel_size = 4\n",
    "        self.stride_size = 2\n",
    "        self.padding_size = 1\n",
    "\n",
    "        self.encoder = self.encoder_block()\n",
    "\n",
    "    def encoder_block(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.out_channels,\n",
    "                kernel_size=self.kernel_size,\n",
    "                stride=self.stride_size,\n",
    "                padding=self.padding_size,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.encoder(x)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Input must be a tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Encoder Block for Variational Autoencoder\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--in_channels\",\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help=\"Number of input channels\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_channels\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Number of output channels\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    in_channels = 3\n",
    "    out_channels = 128\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    for _ in range(2):\n",
    "        layers.append(EncoderBlock(in_channels=in_channels, out_channels=out_channels))\n",
    "        in_channels = out_channels\n",
    "        out_channels //= 2\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "\n",
    "    assert model(torch.randn(1, 3, 256, 256)).size() == (1, 64, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "\n",
    "sys.path.append(\"src/\")\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels=64, out_channels=128):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.kernel_size = 4\n",
    "        self.stride_size = 2\n",
    "        self.padding_size = 1\n",
    "\n",
    "        self.decoder = self.decoder_block()\n",
    "\n",
    "    def decoder_block(self):\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.out_channels,\n",
    "                kernel_size=self.kernel_size,\n",
    "                stride=self.stride_size,\n",
    "                padding=self.padding_size,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.decoder(x)\n",
    "        else:\n",
    "            raise Exception(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Decoder Block for Variational Autoencoder\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--in_channels\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        help=\"Number of input channels\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_channels\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Number of output channels\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    in_channels = 64\n",
    "    out_channels = 128\n",
    "\n",
    "    layers.append(DecoderBlock(in_channels=in_channels, out_channels=out_channels))\n",
    "\n",
    "    in_channels = out_channels\n",
    "\n",
    "    layers.append(\n",
    "        nn.ConvTranspose2d(\n",
    "            in_channels=in_channels, out_channels=3, kernel_size=4, stride=2, padding=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "\n",
    "    assert model(torch.randn(1, 64, 64, 64)).size() == (1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torchview import draw_graph\n",
    "\n",
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, channels=3, image_size=256):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "\n",
    "        self.in_channels = channels\n",
    "        self.out_channels = image_size // 2\n",
    "\n",
    "        self.kernel_size = 3\n",
    "        self.stride_size = 1\n",
    "        self.padding_size = 1\n",
    "\n",
    "        self.encoder_layers = []\n",
    "        self.decoder_layers = []\n",
    "\n",
    "        for _ in range(2):\n",
    "            self.encoder_layers.append(\n",
    "                EncoderBlock(\n",
    "                    in_channels=self.in_channels, out_channels=self.out_channels\n",
    "                )\n",
    "            )\n",
    "            self.in_channels = self.out_channels\n",
    "            self.out_channels //= 2\n",
    "\n",
    "        self.encoder = nn.Sequential(*self.encoder_layers)\n",
    "\n",
    "        self.mean = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=image_size // 4,\n",
    "                out_channels=image_size // 4,\n",
    "                kernel_size=self.kernel_size,\n",
    "                stride=self.stride_size,\n",
    "                padding=self.padding_size,\n",
    "                bias=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.log_variance = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=image_size // 4,\n",
    "                out_channels=image_size // 4,\n",
    "                kernel_size=self.kernel_size,\n",
    "                stride=self.stride_size,\n",
    "                padding=self.padding_size,\n",
    "                bias=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.out_channels = self.in_channels * 2\n",
    "\n",
    "        self.decoder_layers.append(\n",
    "            DecoderBlock(in_channels=self.in_channels, out_channels=self.out_channels)\n",
    "        )\n",
    "        self.decoder_layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=self.out_channels,\n",
    "                    out_channels=channels,\n",
    "                    kernel_size=self.kernel_size + 1,\n",
    "                    stride=self.stride_size + 1,\n",
    "                    padding=self.padding_size,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(*self.decoder_layers)\n",
    "\n",
    "    def reparameterization_trick(self, mean, log_variance):\n",
    "        if isinstance(mean, torch.Tensor) and isinstance(log_variance, torch.Tensor):\n",
    "            standard_deviation = torch.exp(0.5 * log_variance)\n",
    "            eps = torch.randn((standard_deviation.size()))\n",
    "\n",
    "            z = mean + eps * standard_deviation\n",
    "\n",
    "            return z\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Input must be a tensor\".capitalize())\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            encoder = self.encoder(x)\n",
    "\n",
    "            mean = self.mean(encoder)\n",
    "            log_variance = self.log_variance(encoder)\n",
    "\n",
    "            try:\n",
    "                z = self.reparameterization_trick(mean=mean, log_variance=log_variance)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"An error occurred: {}\".format(e))\n",
    "\n",
    "            decoder = self.decoder(z)\n",
    "\n",
    "            return decoder\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Input must be a tensor\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def total_params(model):\n",
    "        if isinstance(model, VariationalAutoEncoder):\n",
    "            return sum(params.numel() for params in model.parameters())\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Input must be a VariationalAutoEncoder\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Model for Variational Autoencoder\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--channels\",\n",
    "        type=int,\n",
    "        default=config()[\"VAE\"][\"channels\"],\n",
    "        help=\"Number of channels in the input image\".title(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_size\",\n",
    "        type=int,\n",
    "        default=config()[\"VAE\"][\"image_size\"],\n",
    "        help=\"Size of the input image\".title(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    variational_autoencoder = VariationalAutoEncoder(\n",
    "        channels=args.channels, image_size=args.image_size\n",
    "    )\n",
    "\n",
    "    assert variational_autoencoder(torch.randn(1, 3, 256, 256)).size() == (\n",
    "        1,\n",
    "        args.channels,\n",
    "        args.image_size,\n",
    "        args.image_size,\n",
    "    )\n",
    "\n",
    "    assert VariationalAutoEncoder.total_params(variational_autoencoder) == 348547\n",
    "\n",
    "    print(summary(model=variational_autoencoder, input_size=(3, 256, 256)))\n",
    "\n",
    "    draw_graph(\n",
    "        model=variational_autoencoder,\n",
    "        input_data=torch.randn(1, args.channels, args.image_size, args.image_size),\n",
    "    ).visual_graph.render(\n",
    "        filename=os.path.join(config()[\"path\"][\"FILES_PATH\"], \"VAE\"), format=\"png\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Model Architecture saved as VAE.png in the path {}\".format(\n",
    "            config()[\"path\"][\"FILES_PATH\"]\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
